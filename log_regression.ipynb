{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "All variable in linear regression should be quantitative. Linear regression is not suitable for classification problem due to be qualitative variables. In many situations, the response variable is qualitative such as yes / no, true / false, mild / moderate / severe etc. Often qualitative variables are referred to as *categorical*. Logistic regression is classification techniques or classifiers which can use to predict a qualitative response. Logistic regression gives you a discrete outcome(cancer or not) but linear regression gives a continuous outcome(the value of car).\n",
    "\n",
    "## Types of Logistic Regression\n",
    "\n",
    "* *Binary Logistic Regression*\n",
    "\n",
    "The categorical response has only two possible outcomes. For example, Fair-Unfair, Agree-Disagree\n",
    "* *Multinomial Logistic Regression*\n",
    "\n",
    "Three or more categories without ordering. For example, Veg, Non-veg, Vegan...\n",
    "* *Ordinal Logistic Regression*\n",
    "\n",
    "Three or more categories with ordering. For example, Movie rating from 1 to 5, Very happy-Quite happy-Neither happy nor unhappy-Unhappy-Very unhappy...\n",
    "\n",
    "## Applications of Logistic Regression\n",
    "- Whether a patient has cancer or not\n",
    "- To predict whether an email is spam or not\n",
    "- To predict whether the tumor is malignantor not\n",
    "- Image Segmentation and Categorization\n",
    "- Handwriting recognition\n",
    "\n",
    "\n",
    "## Function of Logistic Regression \n",
    "\n",
    "\n",
    "Logistic regression measures the relationship between the dependent variable and the one or more independent variables, by estimating probabilities using it's underlying logistic function (sigmoid function)\n",
    "\n",
    " $$ f(t):=\\frac{1}{1+e^{-t}} \\quad \\text{or} \\quad \\frac{e^{t}}{1+e^{t}} $$\n",
    " \n",
    " where $t\\in \\mathbb{R}$ and $f(t)\\in (0,1)$.\n",
    " \n",
    "The sigmoid function is an S-shaped curve that can take any real-valued number and map it into a value between the range of 0 and 1, but never exactly at those boundary values.\n",
    " $$ \\lim_{t\\to\\infty} f(t)=1 \\qquad \\text{and} \\qquad \\lim_{t\\to -\\infty}=0$$\n",
    " \n",
    " <img src=\"files/logisticgraph.png\" width='400' height='300'/>\n",
    " \n",
    " \n",
    " We get the logistic function with respect to the variable $x$ after substituting $t=w\\cdot x + b$ on sigmoid function, \n",
    "$$ p(x)=\\frac{1}{1+e^{-(w\\cdot x +b)}} $$\n",
    "\n",
    "$$P(y=1|\\; x)=p(x)=\\frac{1}{1+e^{-(w\\cdot x +b)}}$$\n",
    "$$P(y=0|\\;x)=1-P(y=1|\\;x)=1-p(x)=\\frac{e^{-(w\\cdot x +b)}}{1+e^{-(w\\cdot x +b)}}$$\n",
    "\n",
    "After taking proportion of the above equation, we get \n",
    "$$\\frac{p}{1-p}=\\frac{P(y=1|\\;x)}{1-P(y=1|\\;x)}=e^{(w\\bullet x +b)}$$\n",
    "Now, take the natural logarithm of the last equation both of sides,\n",
    "$$ ln(\\frac{p}{1-p})=w\\cdot x +b = x'\\cdot \\theta$$\n",
    "\n",
    "where $\\theta=(b,w_1,w_2,\\dots,w_n)$ and $x'=(1,x_1,x_2,\\dots,x_n)$. We are working to find the appropriate $\\theta$ term.\n",
    "\n",
    "\n",
    "$\\frac{p}{1-p}$ is called *odds*, it means that the rate of the probability of event occurrence to the probability of event does not occurrence. The natural logarithm of odds is called *logit*,\n",
    "$$logit(x)=w\\cdot x+b = x'\\cdot \\theta$$\n",
    "\n",
    "\n",
    "The probability that the $x^{(i)}$ data is in class $y^{(i)}=0,1$\n",
    "$$ P(y^{(i)}|x^{(i)}) = p(x^{(i)})^{y^{(i)}}(1-p(x^{(i)})^{1-y^{(i)}} $$\n",
    "where $p(x)=\\frac{1}{1+e^{-(x'\\cdot \\theta)}}$\n",
    "\n",
    "Maximum Likelihood Estimation (MLE) is a general approach to estimating parameters in statistical models.From MLE,\n",
    "$$P(data;\\theta)=\\prod_{(x^{(i)},y^{(i)}\\in data)}p(x^{(i)})^{y^{(i)}}(1-p(x^{(i)}))^{1-y^{(i)}}$$\n",
    "\n",
    "After taking the the natural logarithm\n",
    "$$L(\\theta)=\\ln(P(\\theta))=\\sum_{i=1}^{m} y^{(i)} \\ln(p(x^{(i)};\\theta)) + (1-y^{(i)})\\ln(1-p(x^{(i)};\\theta))$$\n",
    "\n",
    "We want to find $ \\underset{\\theta}{argmax} \\;ln(P(\\theta))$ or $\\underset{\\theta}{argmax}\\; L(\\theta)$\n",
    "\n",
    "We can maximize the likelihood using different methods like Newton's Method or Gradient Descent.\n",
    "\n",
    "If you know the suitable value of $\\theta$, for the given value of the data point $x$ which does not given classify $y^{(i)}$, to decide whether it belongs to $y^{(i)}$ or not, consider\n",
    "$$ P(0|x) = 1-p(x^{(i)}) $$ \n",
    "and\n",
    "$$ P(1|x) = p(x^{(i)}) $$\n",
    "which one is big, it belongs to that class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax Regression \n",
    "\n",
    "Softmax regression (or multinomial logistic regression) is a generalization of logistic regresion that we can use for multi-class classification. In logistic regression, we have binary classification $y^{(i)}=\\{0,1\\}$. In contrast, we have $K$ different classes in multinomial logistic regression such that $y^{(i)}=\\{1,2,\\dots ,K\\}$. Like sigmoid function in logistic regression, we have the *softmax function* such that\n",
    "\n",
    "$$P(y|t^{(i)})=p(t^i)=\\frac{e^{t^{(i)}}}{\\sum_{j=1}^{K} e^{t^{(i)}}} \\qquad\\text{for}\\quad i=1,2,\\dots,K$$\n",
    "\n",
    "Substitute to $t^{(i)}$ we can write $w\\cdot z+b= \\theta \\cdot x$ where  $\\theta=(b,w_1,w_2,\\dots,w_n)$ and $x=(1,z_1,z_2,\\dots,z_n)$.\n",
    "\n",
    "Given a test input $x$, we want to evaluate the probability that $P(y=k|x)$ for each value of $k=1,2,\\dots,K$. Thus, we have the value of the $K$ different probabilities such that our softmax function will be\n",
    "\n",
    "$$p(x;\\theta)=\n",
    "\\begin{bmatrix}\n",
    "P(y=1|x)\\\\\n",
    "P(y=2|x)\\\\\n",
    "\\vdots\\\\\n",
    "P(y=K|x)\n",
    "\\end{bmatrix}=\\frac{1}{\\sum_{j=1}^{K} e^{\\theta^{(i)}\\cdot x}}\n",
    "\\begin{bmatrix}\n",
    "e^{\\theta^{(1)}\\cdot x}\\\\\n",
    "e^{\\theta^{(2)}\\cdot x}\\\\\n",
    "\\vdots\\\\\n",
    "e^{\\theta^{(K)}\\cdot x}\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "where $\\theta^{(i)}\\in \\mathbb{R}^{n+1}$ are the parameters of our model. Notice that $\\sum_{k=1}^{K}P(y=k|x)=1$. The parameter $\\theta$ can be written as\n",
    "\n",
    "$$\n",
    "\\theta=\\begin{bmatrix}\n",
    "|&|&|&|\\\\\n",
    "\\theta^{(1)}&\\theta^{(2)}&\\cdots&\\theta^{(K)}\\\\\n",
    "|&|&|&|\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "The function $L(\\theta)$ in logistic regression can also written as the following form\n",
    "$$\n",
    "L(\\theta)=\\sum_{i=1}^{m}\\sum_{k=0}^{1} 1\\{y^{(i)}=k\\}ln (P(y^{(i)}=k|x^{(i)}))\n",
    "$$\n",
    "where the function $1\\{\\text{a true statement}\\}=1$ and $1\\{\\text{a false statement}\\}=0$.\n",
    "\n",
    "\n",
    "In softmax regression, the function $L(\\theta)$ as the form\n",
    "$$\n",
    "L(\\theta)=\\sum_{i=1}^{m}\\sum_{k=1}^{K} 1\\{y^{(i)}=k\\}ln (P(y^{(i)}=k|x^{(i)}))\n",
    "$$\n",
    "where \n",
    "$$P(y^{(i)}=k|x^{(i)})=\\frac{e^{\\theta^{(k)}\\cdot x^{(i)}}}{\\sum_{j=1}^{K}e^{\\theta^{(j)}\\cdot x^{(i)}}}$$\n",
    "We need to find $\\underset{\\theta}{argmax}\\; L(\\theta)$, we can find it by using optimization methods.\n",
    "## Properties of softmax regression parameterization\n",
    "\n",
    "Suppose that every $\\theta^{(i)}$ is replaced with $\\theta^{(i)}-\\psi$ for ever $i=1,2,\\dots,K$ where $\\psi$ is some fixed vector. Then we obtain\n",
    "$$\n",
    "\\begin{align}\n",
    "P(y^{(i)}=k|x^{(i)};\\theta)&=\\frac{e^{(\\theta^{(k)}-\\psi)\\cdot x^{(i)}}}{\\sum_{j=1}^{K}e^{(\\theta^{(j)}-\\psi)\\cdot x^{(i)}}}\\\\\n",
    "&=\\frac{e^{\\theta^{(k)}\\cdot x^{(i)}}e^{-\\psi\\cdot x^{(i)}}}{\\sum_{j=1}^{K}e^{\\theta^{(j)}\\cdot x^{(i)}}e^{-\\psi\\cdot x^{(i)}}}\\\\\n",
    "&=\\frac{e^{\\theta^{(k)}\\cdot x^{(i)}}}{\\sum_{j=1}^{K}e^{\\theta^{(j)}\\cdot x^{(i)}}}\n",
    "\\end{align}\n",
    "$$\n",
    "This means that our softmax regression model is \"overparameterized\".\n",
    "\n",
    "## Relationship to Logistic Regression\n",
    "\n",
    "In softmax regression, if we take the number of class $K=2$, we get the following function\n",
    "\n",
    "$$\n",
    "p(x ;\\theta)=\\frac{1}{e^{\\theta^{(1)}\\cdot x}+e^{\\theta^{(2)}\\cdot x}}\n",
    "\\begin{bmatrix}\n",
    "e^{\\theta^{(1)}\\cdot x}\\\\\n",
    "e^{\\theta^{(2)}\\cdot x}\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "Since the softmax regression is overparameterized, choose $\\psi=\\theta^{(2)}$ then we got\n",
    "$$\n",
    "\\begin{align}\n",
    "p(x;\\theta)&=\\frac{1}{e^{(\\theta^{(1)}-\\theta^{(2)})\\cdot x}+e^{(\\theta^{(2)}-\\theta^{(2)})\\cdot x}}\n",
    "\\begin{bmatrix}\n",
    "e^{(\\theta^{(1)}-\\theta^{(2)})\\cdot x}\\\\\n",
    "e^{(\\theta^{(2)}-\\theta^{(2)})\\cdot x}\n",
    "\\end{bmatrix}\\\\\n",
    "&=\\begin{bmatrix}\n",
    "\\frac{e^{\\theta'\\cdot x}}{e^{\\theta'\\cdot x}+e^{\\vec{0}\\cdot x}}\\\\\n",
    "\\frac{e^{\\vec{0}\\cdot x}}{e^{\\theta'\\cdot x}+e^{\\vec{0}\\cdot x}}\n",
    "\\end{bmatrix}\\\\\n",
    "&=\\begin{bmatrix}\n",
    "\\frac{1}{1+e^{-\\theta'\\cdot x}}\\\\\n",
    "\\frac{e^{-\\theta'\\cdot x}}{1+e^{-\\theta'\\cdot x}}\n",
    "\\end{bmatrix}\\\\\n",
    "&=\\begin{bmatrix}\n",
    "\\frac{1}{1+e^{-\\theta'\\cdot x}}\\\\\n",
    "1-\\frac{1}{1+e^{-\\theta'\\cdot x}}\n",
    "\\end{bmatrix}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where the parameter $\\theta':= \\theta^{(1)}-\\theta^{(2)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "1) James, G., Witten, D., Hastie, T., & Tibshirani, R. (2013). An introduction to statistical learning (Vol. 112). New York: springer.\n",
    "\n",
    "2) https://machinelearning-blog.com/2018/04/23/logistic-regression-101/\n",
    "\n",
    "3) http://ufldl.stanford.edu/tutorial/supervised/SoftmaxRegression/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sorular\n",
    "1) softmax regression nın en öenmli özelliği olan \"overparameterized\"  ne demek ? Formal olarak rastgele bi parametre aldığımızda hiç bir etkisi olmuyor ama bu bize ne kazandırıyor ?\n",
    "\n",
    "2) softmax functionu nasıl elde ediyoruz. Sigma functiondan elde edilişi var mıdır ?\n",
    "\n",
    "3)  Eksiğim kalmış mı hocam? Bu çalışmalardan sonraki aşamam nedir hocam ?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
